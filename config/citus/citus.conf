# Citus Configuration
# PostgreSQL configuration optimized for Citus distributed database
# Compatible with RuVector extension for vector operations

# ============================================
# CITUS SETTINGS
# ============================================

# Enable Citus extension (loaded via shared_preload_libraries)
# shared_preload_libraries = 'citus,ruvector'

# Shard placement settings
citus.shard_count = 32                    # Default number of shards for distributed tables
citus.shard_replication_factor = 1        # Number of shard replicas (1 = no replication)

# Query routing and execution
citus.multi_shard_commit_protocol = '2pc' # Two-phase commit for multi-shard transactions
citus.enable_repartition_joins = on       # Allow joins that require repartitioning
citus.enable_binary_protocol = on        # Use binary protocol for worker communication
citus.binary_worker_copy_format = on     # Binary format for COPY operations

# Parallel query execution
citus.max_adaptive_executor_pool_size = 16  # Max parallel connections per worker
citus.executor_slow_start_interval = 10     # Milliseconds to wait between connection openings
citus.max_cached_conns_per_worker = 2       # Cached connections per worker node

# Distributed deadlock detection
citus.distributed_deadlock_detection_factor = 2  # Multiplier for deadlock timeout
citus.log_distributed_deadlock_detection = on    # Log deadlock detection events

# Statistics and monitoring
citus.stat_statements_track = all         # Track all statements (all/none/top)
citus.stat_tenants_track = all           # Track tenant-level statistics
citus.stat_tenants_period = 10000        # Statistics collection period (ms)
citus.stat_tenants_log_level = 'log'    # Log level for tenant stats

# Task execution
citus.task_executor_type = 'adaptive'    # adaptive/real-time/task-tracker
citus.multi_task_query_log_level = 'log' # Log level for multi-task queries
citus.task_assignment_policy = 'greedy'  # greedy/round-robin/first-replica

# Connection management
citus.node_connection_timeout = 30000    # Connection timeout in milliseconds
citus.max_shared_pool_size = 100         # Max shared connection pool size
citus.node_conninfo_timeout = 10000      # Metadata cache timeout (ms)

# Replication and failure handling
citus.force_max_query_parallelization = off  # Force max parallelization (debug only)
citus.task_tracker_delay = 200               # Delay between task assignments (ms)
citus.max_assign_task_batch_size = 64        # Max tasks in a batch

# ============================================
# POSTGRESQL SETTINGS (Citus-optimized)
# ============================================

# Memory settings
shared_buffers = 1GB                     # 25% of RAM for coordinator, more for workers
effective_cache_size = 3GB               # 75% of RAM
work_mem = 64MB                          # Per operation memory
maintenance_work_mem = 512MB             # For VACUUM, CREATE INDEX

# Connection settings
max_connections = 200                    # Higher for workers to handle coordinator connections
superuser_reserved_connections = 5

# WAL settings (for logical replication support)
wal_level = logical                      # Required for Citus
max_wal_senders = 10
max_replication_slots = 10
max_worker_processes = 16                # Background workers for parallel queries

# Query planning
random_page_cost = 1.1                   # SSD optimized
effective_io_concurrency = 200           # SSD optimized
max_parallel_workers_per_gather = 4
max_parallel_workers = 8

# Logging (for debugging distributed queries)
log_min_duration_statement = 1000        # Log queries slower than 1s
log_line_prefix = '%m [%p] %q%u@%d '    # Include timestamp, process, user, database
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on
log_statement = 'none'                   # none/ddl/mod/all (none for production)

# Statistics
shared_preload_libraries = 'citus,ruvector,pg_stat_statements'
pg_stat_statements.max = 10000
pg_stat_statements.track = all
track_activities = on
track_counts = on
track_io_timing = on

# ============================================
# RUVECTOR SETTINGS
# ============================================

# HNSW index settings (for vector similarity search)
# These are set during index creation, but can be tuned here
# ruvector.hnsw_m = 16                   # Number of connections per layer
# ruvector.hnsw_ef_construction = 100    # Size of dynamic candidate list during construction
# ruvector.hnsw_ef_search = 40           # Size of dynamic candidate list during search

# ============================================
# COORDINATOR-SPECIFIC SETTINGS
# ============================================
# These settings should be used on the coordinator node only

# Coordinator should have lower memory allocations than workers
# shared_buffers = 512MB
# effective_cache_size = 1536MB
# work_mem = 32MB
# maintenance_work_mem = 256MB
# max_connections = 100

# ============================================
# WORKER-SPECIFIC SETTINGS
# ============================================
# These settings should be used on worker nodes

# Workers should have higher memory allocations for data processing
# shared_buffers = 2GB
# effective_cache_size = 6GB
# work_mem = 128MB
# maintenance_work_mem = 1GB
# max_connections = 200

# ============================================
# SHARDING STRATEGY NOTES
# ============================================
#
# Hash Sharding (recommended for most use cases):
#   SELECT create_distributed_table('table_name', 'distribution_column');
#   - Evenly distributes data across shards based on hash of distribution column
#   - Best for large tables with uniform access patterns
#   - Supports co-location for joins (colocate_with parameter)
#
# Range Sharding (for time-series or sequential data):
#   SELECT create_distributed_table('table_name', 'distribution_column', 'range');
#   - Distributes data based on ranges of distribution column values
#   - Good for time-series data or ordered data
#   - Allows efficient pruning for range queries
#
# Reference Tables (small lookup tables):
#   SELECT create_reference_table('table_name');
#   - Replicates entire table to all nodes
#   - Best for small, frequently joined tables (< 1GB)
#   - No distribution column required
#
# Append Tables (for append-only workloads):
#   SELECT create_distributed_table('table_name', 'distribution_column', 'append');
#   - Optimized for append-only workloads (logs, events)
#   - Shards are immutable after creation
#   - Can be used with time-based partitioning

# ============================================
# PERFORMANCE TUNING TIPS
# ============================================
#
# 1. Co-locate related tables for faster joins:
#    SELECT create_distributed_table('orders', 'customer_id', colocate_with => 'customers');
#
# 2. Use reference tables for small dimension tables:
#    SELECT create_reference_table('countries');
#
# 3. Increase shard count for large tables (trade-off: more shards = more overhead):
#    SELECT create_distributed_table('big_table', 'id', shard_count => 64);
#
# 4. Use EXPLAIN (ANALYZE, DIST) to see query distribution:
#    EXPLAIN (ANALYZE, DIST) SELECT * FROM distributed_table WHERE id = 123;
#
# 5. Monitor shard distribution:
#    SELECT * FROM citus_shards;
#    SELECT nodename, count(*) FROM citus_shards GROUP BY nodename;
#
# 6. Rebalance shards when adding/removing workers:
#    SELECT rebalance_table_shards('table_name');
#
# 7. Use pg_stat_statements to identify slow queries:
#    SELECT query, mean_exec_time FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 10;

# ============================================
# MONITORING QUERIES
# ============================================
#
# View active worker nodes:
#   SELECT * FROM citus_get_active_worker_nodes();
#
# View distributed tables:
#   SELECT * FROM citus_tables;
#
# View shard distribution:
#   SELECT * FROM citus_shards;
#
# View shard sizes:
#   SELECT
#       nodename,
#       pg_size_pretty(SUM(shard_size)) as total_size,
#       COUNT(*) as shard_count
#   FROM citus_shards
#   GROUP BY nodename;
#
# View distributed queries:
#   SELECT * FROM citus_dist_stat_activity;
#
# Check cluster health:
#   SELECT * FROM citus_check_cluster_node_health();
