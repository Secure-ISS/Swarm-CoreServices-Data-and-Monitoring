version: '3.8'

# Distributed PostgreSQL Mesh with RuVector - Docker Swarm Stack
# Topology: 1 Coordinator + N Workers with Connection Pooling

networks:
  postgres_mesh:
    driver: overlay
    attachable: true
    driver_opts:
      encrypted: "true"
    ipam:
      config:
        - subnet: 10.0.10.0/24

volumes:
  coordinator_data:
    driver: local
  worker_data_1:
    driver: local
  worker_data_2:
    driver: local
  worker_data_3:
    driver: local
  pgbouncer_config:
    driver: local
  backup_storage:
    driver: local

secrets:
  postgres_password:
    external: true
  replication_password:
    external: true
  pgbouncer_auth:
    external: true

configs:
  coordinator_config:
    file: ./configs/coordinator.conf
  worker_config:
    file: ./configs/worker.conf
  pgbouncer_config:
    file: ./configs/pgbouncer.ini

services:
  # ==============================================
  # COORDINATOR NODE (Query Router & Metadata)
  # ==============================================
  coordinator:
    image: ruvnet/ruvector-postgres:latest
    hostname: pg-coordinator
    networks:
      postgres_mesh:
        aliases:
          - postgres-coordinator
    ports:
      - target: 5432
        published: 5432
        protocol: tcp
        mode: ingress
    volumes:
      - coordinator_data:/var/lib/postgresql/data
      - type: bind
        source: ./init-scripts
        target: /docker-entrypoint-initdb.d
        read_only: true
    secrets:
      - source: postgres_password
        target: postgres_password
      - source: replication_password
        target: replication_password
    configs:
      - source: coordinator_config
        target: /etc/postgresql/postgresql.conf
    environment:
      POSTGRES_DB: distributed_postgres_cluster
      POSTGRES_USER: dpg_cluster
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_INITDB_ARGS: "-E UTF8 --locale=en_US.utf8"
      # Coordinator-specific settings
      POSTGRES_ROLE: coordinator
      POSTGRES_MAX_CONNECTIONS: 500
      POSTGRES_SHARED_BUFFERS: 4GB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 12GB
      POSTGRES_WORK_MEM: 64MB
      POSTGRES_MAINTENANCE_WORK_MEM: 1GB
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
          - node.labels.postgres.role == coordinator
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 5
        window: 120s
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        order: stop-first
      labels:
        - "com.distributed-postgres.role=coordinator"
        - "com.distributed-postgres.cluster=main"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dpg_cluster -d distributed_postgres_cluster"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s

  # ==============================================
  # WORKER NODES (Data Storage & Processing)
  # ==============================================
  worker-1:
    image: ruvnet/ruvector-postgres:latest
    hostname: pg-worker-1
    networks:
      postgres_mesh:
        aliases:
          - postgres-worker-1
    volumes:
      - worker_data_1:/var/lib/postgresql/data
    secrets:
      - source: postgres_password
        target: postgres_password
      - source: replication_password
        target: replication_password
    configs:
      - source: worker_config
        target: /etc/postgresql/postgresql.conf
    environment:
      POSTGRES_DB: distributed_postgres_cluster
      POSTGRES_USER: dpg_cluster
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
      PGDATA: /var/lib/postgresql/data/pgdata
      # Worker-specific settings
      POSTGRES_ROLE: worker
      POSTGRES_COORDINATOR_HOST: pg-coordinator
      POSTGRES_COORDINATOR_PORT: 5432
      POSTGRES_MAX_CONNECTIONS: 200
      POSTGRES_SHARED_BUFFERS: 2GB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 6GB
      POSTGRES_WORK_MEM: 32MB
      POSTGRES_MAINTENANCE_WORK_MEM: 512MB
      # Replication settings
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD_FILE: /run/secrets/replication_password
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
          - node.labels.postgres.role == worker
        preferences:
          - spread: node.id
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 5
        window: 120s
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        order: start-first
      labels:
        - "com.distributed-postgres.role=worker"
        - "com.distributed-postgres.cluster=main"
        - "com.distributed-postgres.worker-id=1"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dpg_cluster -d distributed_postgres_cluster"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 60s

  worker-2:
    image: ruvnet/ruvector-postgres:latest
    hostname: pg-worker-2
    networks:
      postgres_mesh:
        aliases:
          - postgres-worker-2
    volumes:
      - worker_data_2:/var/lib/postgresql/data
    secrets:
      - source: postgres_password
        target: postgres_password
      - source: replication_password
        target: replication_password
    configs:
      - source: worker_config
        target: /etc/postgresql/postgresql.conf
    environment:
      POSTGRES_DB: distributed_postgres_cluster
      POSTGRES_USER: dpg_cluster
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_ROLE: worker
      POSTGRES_COORDINATOR_HOST: pg-coordinator
      POSTGRES_COORDINATOR_PORT: 5432
      POSTGRES_MAX_CONNECTIONS: 200
      POSTGRES_SHARED_BUFFERS: 2GB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 6GB
      POSTGRES_WORK_MEM: 32MB
      POSTGRES_MAINTENANCE_WORK_MEM: 512MB
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD_FILE: /run/secrets/replication_password
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
          - node.labels.postgres.role == worker
        preferences:
          - spread: node.id
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 5
        window: 120s
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        order: start-first
      labels:
        - "com.distributed-postgres.role=worker"
        - "com.distributed-postgres.cluster=main"
        - "com.distributed-postgres.worker-id=2"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dpg_cluster -d distributed_postgres_cluster"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 60s

  worker-3:
    image: ruvnet/ruvector-postgres:latest
    hostname: pg-worker-3
    networks:
      postgres_mesh:
        aliases:
          - postgres-worker-3
    volumes:
      - worker_data_3:/var/lib/postgresql/data
    secrets:
      - source: postgres_password
        target: postgres_password
      - source: replication_password
        target: replication_password
    configs:
      - source: worker_config
        target: /etc/postgresql/postgresql.conf
    environment:
      POSTGRES_DB: distributed_postgres_cluster
      POSTGRES_USER: dpg_cluster
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_ROLE: worker
      POSTGRES_COORDINATOR_HOST: pg-coordinator
      POSTGRES_COORDINATOR_PORT: 5432
      POSTGRES_MAX_CONNECTIONS: 200
      POSTGRES_SHARED_BUFFERS: 2GB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 6GB
      POSTGRES_WORK_MEM: 32MB
      POSTGRES_MAINTENANCE_WORK_MEM: 512MB
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD_FILE: /run/secrets/replication_password
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
          - node.labels.postgres.role == worker
        preferences:
          - spread: node.id
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 5
        window: 120s
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        order: start-first
      labels:
        - "com.distributed-postgres.role=worker"
        - "com.distributed-postgres.cluster=main"
        - "com.distributed-postgres.worker-id=3"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dpg_cluster -d distributed_postgres_cluster"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 60s

  # ==============================================
  # CONNECTION POOLER (PgBouncer)
  # ==============================================
  pgbouncer:
    image: edoburu/pgbouncer:latest
    hostname: pgbouncer
    networks:
      postgres_mesh:
        aliases:
          - postgres-pooler
    ports:
      - target: 6432
        published: 6432
        protocol: tcp
        mode: ingress
    volumes:
      - pgbouncer_config:/etc/pgbouncer
    secrets:
      - source: pgbouncer_auth
        target: /etc/pgbouncer/userlist.txt
    configs:
      - source: pgbouncer_config
        target: /etc/pgbouncer/pgbouncer.ini
    environment:
      DATABASE_URL: "postgresql://dpg_cluster@pg-coordinator:5432/distributed_postgres_cluster"
      POOL_MODE: transaction
      MAX_CLIENT_CONN: 1000
      DEFAULT_POOL_SIZE: 25
      MIN_POOL_SIZE: 10
      RESERVE_POOL_SIZE: 5
      SERVER_IDLE_TIMEOUT: 600
      SERVER_LIFETIME: 3600
      SERVER_CONNECT_TIMEOUT: 15
      QUERY_TIMEOUT: 300
      QUERY_WAIT_TIMEOUT: 120
    deploy:
      mode: replicated
      replicas: 2
      placement:
        constraints:
          - node.role == worker
        preferences:
          - spread: node.id
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 60s
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        order: start-first
      labels:
        - "com.distributed-postgres.role=pooler"
        - "com.distributed-postgres.cluster=main"
    healthcheck:
      test: ["CMD-SHELL", "psql -U dpg_cluster -h localhost -p 6432 -d distributed_postgres_cluster -c 'SELECT 1'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ==============================================
  # HEALTH MONITORING SERVICE
  # ==============================================
  health-monitor:
    image: python:3.11-slim
    hostname: health-monitor
    networks:
      - postgres_mesh
    volumes:
      - type: bind
        source: ./scripts
        target: /scripts
        read_only: true
      - type: bind
        source: ./logs
        target: /logs
    environment:
      COORDINATOR_HOST: pg-coordinator
      COORDINATOR_PORT: 5432
      WORKER_HOSTS: pg-worker-1,pg-worker-2,pg-worker-3
      POOLER_HOST: pgbouncer
      POOLER_PORT: 6432
      CHECK_INTERVAL: 60
      LOG_LEVEL: INFO
    command: >
      sh -c "pip install psycopg2-binary requests &&
             python /scripts/health-monitor.py"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
        window: 120s
      labels:
        - "com.distributed-postgres.role=monitor"
        - "com.distributed-postgres.cluster=main"
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f health-monitor.py || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ==============================================
  # BACKUP SERVICE
  # ==============================================
  backup-agent:
    image: ruvnet/ruvector-postgres:latest
    hostname: backup-agent
    networks:
      - postgres_mesh
    volumes:
      - backup_storage:/backups
      - type: bind
        source: ./scripts
        target: /scripts
        read_only: true
    secrets:
      - source: postgres_password
        target: postgres_password
    environment:
      COORDINATOR_HOST: pg-coordinator
      COORDINATOR_PORT: 5432
      POSTGRES_USER: dpg_cluster
      POSTGRES_DB: distributed_postgres_cluster
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
      BACKUP_SCHEDULE: "0 2 * * *"  # Daily at 2 AM
      BACKUP_RETENTION_DAYS: 7
      BACKUP_PATH: /backups
    command: >
      sh -c "apt-get update && apt-get install -y cron &&
             crontab -l | { cat; echo '$$BACKUP_SCHEDULE /scripts/backup-distributed.sh'; } | crontab - &&
             cron -f"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
        delay: 60s
        max_attempts: 3
        window: 300s
      labels:
        - "com.distributed-postgres.role=backup"
        - "com.distributed-postgres.cluster=main"
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f crond || exit 1"]
      interval: 300s
      timeout: 10s
      retries: 2
      start_period: 60s
