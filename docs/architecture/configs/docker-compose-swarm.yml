version: '3.8'

# Distributed PostgreSQL Cluster Stack for Docker Swarm
# Hybrid Citus + Patroni architecture with RuVector support

networks:
  coordinator-net:
    driver: overlay
    attachable: true
    ipam:
      config:
        - subnet: 10.0.1.0/26
  worker-net:
    driver: overlay
    attachable: true
    ipam:
      config:
        - subnet: 10.0.1.64/26
  admin-net:
    driver: overlay
    attachable: true
    ipam:
      config:
        - subnet: 10.0.1.128/26

volumes:
  coordinator-1-data:
  coordinator-2-data:
  coordinator-3-data:
  worker-1-1-data:
  worker-1-2-data:
  worker-2-1-data:
  worker-2-2-data:
  worker-3-1-data:
  worker-3-2-data:
  etcd-1-data:
  etcd-2-data:
  etcd-3-data:

secrets:
  postgres_password:
    external: true
  mcp_password:
    external: true
  replication_password:
    external: true

configs:
  patroni-coordinator:
    file: ./patroni-coordinator.yml
  patroni-worker:
    file: ./patroni-worker.yml
  haproxy-cfg:
    file: ./haproxy.cfg
  pgbouncer-ini:
    file: ./pgbouncer.ini

services:
  # ============================================================
  # etcd Cluster (Distributed Consensus)
  # ============================================================
  etcd-1:
    image: quay.io/coreos/etcd:v3.5.11
    hostname: etcd-1
    networks:
      - coordinator-net
    volumes:
      - etcd-1-data:/etcd-data
    environment:
      ETCD_NAME: etcd-1
      ETCD_DATA_DIR: /etcd-data
      ETCD_LISTEN_CLIENT_URLS: http://0.0.0.0:2379
      ETCD_ADVERTISE_CLIENT_URLS: http://etcd-1:2379
      ETCD_LISTEN_PEER_URLS: http://0.0.0.0:2380
      ETCD_INITIAL_ADVERTISE_PEER_URLS: http://etcd-1:2380
      ETCD_INITIAL_CLUSTER: etcd-1=http://etcd-1:2380,etcd-2=http://etcd-2:2380,etcd-3=http://etcd-3:2380
      ETCD_INITIAL_CLUSTER_STATE: new
      ETCD_INITIAL_CLUSTER_TOKEN: postgres-cluster-token
      ETCD_HEARTBEAT_INTERVAL: 100
      ETCD_ELECTION_TIMEOUT: 1000
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
        max_replicas_per_node: 1
      restart_policy:
        condition: on-failure
        delay: 10s

  etcd-2:
    image: quay.io/coreos/etcd:v3.5.11
    hostname: etcd-2
    networks:
      - coordinator-net
    volumes:
      - etcd-2-data:/etcd-data
    environment:
      ETCD_NAME: etcd-2
      ETCD_DATA_DIR: /etcd-data
      ETCD_LISTEN_CLIENT_URLS: http://0.0.0.0:2379
      ETCD_ADVERTISE_CLIENT_URLS: http://etcd-2:2379
      ETCD_LISTEN_PEER_URLS: http://0.0.0.0:2380
      ETCD_INITIAL_ADVERTISE_PEER_URLS: http://etcd-2:2380
      ETCD_INITIAL_CLUSTER: etcd-1=http://etcd-1:2380,etcd-2=http://etcd-2:2380,etcd-3=http://etcd-3:2380
      ETCD_INITIAL_CLUSTER_STATE: new
      ETCD_INITIAL_CLUSTER_TOKEN: postgres-cluster-token
      ETCD_HEARTBEAT_INTERVAL: 100
      ETCD_ELECTION_TIMEOUT: 1000
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
        max_replicas_per_node: 1
      restart_policy:
        condition: on-failure
        delay: 10s

  etcd-3:
    image: quay.io/coreos/etcd:v3.5.11
    hostname: etcd-3
    networks:
      - coordinator-net
    volumes:
      - etcd-3-data:/etcd-data
    environment:
      ETCD_NAME: etcd-3
      ETCD_DATA_DIR: /etcd-data
      ETCD_LISTEN_CLIENT_URLS: http://0.0.0.0:2379
      ETCD_ADVERTISE_CLIENT_URLS: http://etcd-3:2379
      ETCD_LISTEN_PEER_URLS: http://0.0.0.0:2380
      ETCD_INITIAL_ADVERTISE_PEER_URLS: http://etcd-3:2380
      ETCD_INITIAL_CLUSTER: etcd-1=http://etcd-1:2380,etcd-2=http://etcd-2:2380,etcd-3=http://etcd-3:2380
      ETCD_INITIAL_CLUSTER_STATE: new
      ETCD_INITIAL_CLUSTER_TOKEN: postgres-cluster-token
      ETCD_HEARTBEAT_INTERVAL: 100
      ETCD_ELECTION_TIMEOUT: 1000
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
        max_replicas_per_node: 1
      restart_policy:
        condition: on-failure
        delay: 10s

  # ============================================================
  # Coordinators (Citus + Patroni + RuVector)
  # ============================================================
  coordinator-1:
    image: ruvnet/ruvector-postgres:latest
    hostname: coordinator-1
    networks:
      - coordinator-net
      - admin-net
    volumes:
      - coordinator-1-data:/var/lib/postgresql/data
    environment:
      PATRONI_NAME: coordinator-1
      PATRONI_SCOPE: postgres-cluster-coordinators
      PATRONI_ETCD3_HOSTS: etcd-1:2379,etcd-2:2379,etcd-3:2379
      PATRONI_POSTGRESQL_CONNECT_ADDRESS: coordinator-1:5432
      PATRONI_RESTAPI_CONNECT_ADDRESS: coordinator-1:8008
      PATRONI_POSTGRESQL_DATA_DIR: /var/lib/postgresql/data
      PATRONI_REPLICATION_USERNAME: replicator
      PATRONI_REPLICATION_PASSWORD_FILE: /run/secrets/replication_password
      PATRONI_SUPERUSER_USERNAME: postgres
      PATRONI_SUPERUSER_PASSWORD_FILE: /run/secrets/postgres_password
      PATRONI_CITUS_COORDINATOR: "true"
      PATRONI_POSTGRESQL_PARAMETERS_SHARED_PRELOAD_LIBRARIES: citus,ruvector
      PATRONI_POSTGRESQL_PARAMETERS_MAX_CONNECTIONS: "100"
      PATRONI_POSTGRESQL_PARAMETERS_SHARED_BUFFERS: 2GB
      PATRONI_POSTGRESQL_PARAMETERS_EFFECTIVE_CACHE_SIZE: 6GB
      PATRONI_POSTGRESQL_PARAMETERS_WORK_MEM: 64MB
      PATRONI_POSTGRESQL_PARAMETERS_MAINTENANCE_WORK_MEM: 512MB
      PATRONI_POSTGRESQL_PARAMETERS_WAL_LEVEL: replica
      PATRONI_POSTGRESQL_PARAMETERS_SYNCHRONOUS_COMMIT: "on"
      PATRONI_POSTGRESQL_PARAMETERS_SYNCHRONOUS_STANDBY_NAMES: "*"
      PATRONI_POSTGRESQL_PARAMETERS_MAX_WAL_SENDERS: 10
      PATRONI_POSTGRESQL_PARAMETERS_MAX_REPLICATION_SLOTS: 10
    secrets:
      - postgres_password
      - replication_password
    configs:
      - source: patroni-coordinator
        target: /etc/patroni.yml
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.postgres_role == coordinator
        max_replicas_per_node: 1
      resources:
        limits:
          cpus: '2'
          memory: 8G
        reservations:
          cpus: '1'
          memory: 4G
      restart_policy:
        condition: on-failure
        delay: 10s

  coordinator-2:
    image: ruvnet/ruvector-postgres:latest
    hostname: coordinator-2
    networks:
      - coordinator-net
      - admin-net
    volumes:
      - coordinator-2-data:/var/lib/postgresql/data
    environment:
      PATRONI_NAME: coordinator-2
      PATRONI_SCOPE: postgres-cluster-coordinators
      PATRONI_ETCD3_HOSTS: etcd-1:2379,etcd-2:2379,etcd-3:2379
      PATRONI_POSTGRESQL_CONNECT_ADDRESS: coordinator-2:5432
      PATRONI_RESTAPI_CONNECT_ADDRESS: coordinator-2:8008
      PATRONI_POSTGRESQL_DATA_DIR: /var/lib/postgresql/data
      PATRONI_REPLICATION_USERNAME: replicator
      PATRONI_REPLICATION_PASSWORD_FILE: /run/secrets/replication_password
      PATRONI_SUPERUSER_USERNAME: postgres
      PATRONI_SUPERUSER_PASSWORD_FILE: /run/secrets/postgres_password
      PATRONI_CITUS_COORDINATOR: "true"
      PATRONI_POSTGRESQL_PARAMETERS_SHARED_PRELOAD_LIBRARIES: citus,ruvector
      PATRONI_POSTGRESQL_PARAMETERS_MAX_CONNECTIONS: "100"
      PATRONI_POSTGRESQL_PARAMETERS_SHARED_BUFFERS: 2GB
      PATRONI_POSTGRESQL_PARAMETERS_EFFECTIVE_CACHE_SIZE: 6GB
    secrets:
      - postgres_password
      - replication_password
    configs:
      - source: patroni-coordinator
        target: /etc/patroni.yml
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.postgres_role == coordinator
        max_replicas_per_node: 1
      resources:
        limits:
          cpus: '2'
          memory: 8G
        reservations:
          cpus: '1'
          memory: 4G
      restart_policy:
        condition: on-failure
        delay: 10s

  coordinator-3:
    image: ruvnet/ruvector-postgres:latest
    hostname: coordinator-3
    networks:
      - coordinator-net
      - admin-net
    volumes:
      - coordinator-3-data:/var/lib/postgresql/data
    environment:
      PATRONI_NAME: coordinator-3
      PATRONI_SCOPE: postgres-cluster-coordinators
      PATRONI_ETCD3_HOSTS: etcd-1:2379,etcd-2:2379,etcd-3:2379
      PATRONI_POSTGRESQL_CONNECT_ADDRESS: coordinator-3:5432
      PATRONI_RESTAPI_CONNECT_ADDRESS: coordinator-3:8008
      PATRONI_POSTGRESQL_DATA_DIR: /var/lib/postgresql/data
      PATRONI_REPLICATION_USERNAME: replicator
      PATRONI_REPLICATION_PASSWORD_FILE: /run/secrets/replication_password
      PATRONI_SUPERUSER_USERNAME: postgres
      PATRONI_SUPERUSER_PASSWORD_FILE: /run/secrets/postgres_password
      PATRONI_CITUS_COORDINATOR: "true"
      PATRONI_POSTGRESQL_PARAMETERS_SHARED_PRELOAD_LIBRARIES: citus,ruvector
      PATRONI_POSTGRESQL_PARAMETERS_MAX_CONNECTIONS: "100"
      PATRONI_POSTGRESQL_PARAMETERS_SHARED_BUFFERS: 2GB
      PATRONI_POSTGRESQL_PARAMETERS_EFFECTIVE_CACHE_SIZE: 6GB
    secrets:
      - postgres_password
      - replication_password
    configs:
      - source: patroni-coordinator
        target: /etc/patroni.yml
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.postgres_role == coordinator
        max_replicas_per_node: 1
      resources:
        limits:
          cpus: '2'
          memory: 8G
        reservations:
          cpus: '1'
          memory: 4G
      restart_policy:
        condition: on-failure
        delay: 10s

  # ============================================================
  # Workers (Citus Workers + Patroni + RuVector)
  # ============================================================
  worker-1-1:
    image: ruvnet/ruvector-postgres:latest
    hostname: worker-1-1
    networks:
      - worker-net
    volumes:
      - worker-1-1-data:/var/lib/postgresql/data
    environment:
      PATRONI_NAME: worker-1-1
      PATRONI_SCOPE: postgres-cluster-shard-1
      PATRONI_ETCD3_HOSTS: etcd-1:2379,etcd-2:2379,etcd-3:2379
      PATRONI_POSTGRESQL_CONNECT_ADDRESS: worker-1-1:5432
      PATRONI_RESTAPI_CONNECT_ADDRESS: worker-1-1:8008
      PATRONI_CITUS_WORKER: "true"
      PATRONI_POSTGRESQL_PARAMETERS_SHARED_PRELOAD_LIBRARIES: citus,ruvector
      PATRONI_POSTGRESQL_PARAMETERS_MAX_CONNECTIONS: "200"
      PATRONI_POSTGRESQL_PARAMETERS_SHARED_BUFFERS: 4GB
      PATRONI_POSTGRESQL_PARAMETERS_EFFECTIVE_CACHE_SIZE: 12GB
      PATRONI_POSTGRESQL_PARAMETERS_WORK_MEM: 128MB
      PATRONI_POSTGRESQL_PARAMETERS_SYNCHRONOUS_COMMIT: "local"
    secrets:
      - postgres_password
      - replication_password
    configs:
      - source: patroni-worker
        target: /etc/patroni.yml
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 2
      resources:
        limits:
          cpus: '4'
          memory: 16G
        reservations:
          cpus: '2'
          memory: 8G
      restart_policy:
        condition: on-failure
        delay: 10s

  worker-1-2:
    image: ruvnet/ruvector-postgres:latest
    hostname: worker-1-2
    networks:
      - worker-net
    volumes:
      - worker-1-2-data:/var/lib/postgresql/data
    environment:
      PATRONI_NAME: worker-1-2
      PATRONI_SCOPE: postgres-cluster-shard-1
      PATRONI_ETCD3_HOSTS: etcd-1:2379,etcd-2:2379,etcd-3:2379
      PATRONI_POSTGRESQL_CONNECT_ADDRESS: worker-1-2:5432
      PATRONI_RESTAPI_CONNECT_ADDRESS: worker-1-2:8008
      PATRONI_CITUS_WORKER: "true"
      PATRONI_POSTGRESQL_PARAMETERS_SHARED_PRELOAD_LIBRARIES: citus,ruvector
    secrets:
      - postgres_password
      - replication_password
    configs:
      - source: patroni-worker
        target: /etc/patroni.yml
    deploy:
      replicas: 1
      placement:
        max_replicas_per_node: 2
      resources:
        limits:
          cpus: '4'
          memory: 16G
        reservations:
          cpus: '2'
          memory: 8G
      restart_policy:
        condition: on-failure
        delay: 10s

  # Additional workers (2-1, 2-2, 3-1, 3-2) - similar configuration
  # Omitted for brevity, follow same pattern with different PATRONI_SCOPE

  # ============================================================
  # HAProxy (Load Balancer)
  # ============================================================
  haproxy:
    image: haproxy:2.9-alpine
    hostname: haproxy
    networks:
      - admin-net
      - coordinator-net
    ports:
      - target: 5432
        published: 5432
        protocol: tcp
        mode: ingress
      - target: 8404
        published: 8404
        protocol: tcp
        mode: ingress  # Stats page
    configs:
      - source: haproxy-cfg
        target: /usr/local/etc/haproxy/haproxy.cfg
    deploy:
      replicas: 2
      placement:
        max_replicas_per_node: 1
      restart_policy:
        condition: on-failure
        delay: 5s

  # ============================================================
  # Monitoring (Optional)
  # ============================================================
  prometheus:
    image: prom/prometheus:latest
    networks:
      - admin-net
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager

  grafana:
    image: grafana/grafana:latest
    networks:
      - admin-net
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
